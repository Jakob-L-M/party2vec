{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Notebook is used to scrape all used data\n",
    "You may consider it the root of the project.\n",
    "\n",
    "To be able to run it, the 'party-frames' have to already be created. They are used to get the usernames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_party_name(path):\n",
    "    path = path[path.rindex('\\\\') + 1:]\n",
    "    return path[:path.index('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('..\\\\party-frames\\\\' + '*.csv')\n",
    "\n",
    "# Here we can save profiles that already have been scraped, so that the scraping doesnt have to be done in one sitting\n",
    "profiles_scraped = []\n",
    "\n",
    "for file in files:\n",
    "    # Get the party name (like FDP, CDU or AfD) from the party-frames document name\n",
    "    party = extract_party_name(file)\n",
    "    # Select the top 15 politicans with the most followers\n",
    "    frame = pd.read_csv(file).head(15)\n",
    "    \n",
    "    if not os.path.exists('./twitter-data/' + party):\n",
    "        # Create a new folder if theres isnt one already\n",
    "        os.makedirs('./twitter-data/' + party)\n",
    "    \n",
    "    for profile in frame['profile_link'].to_numpy():\n",
    "        link = profile\n",
    "        link = link[link.rindex('/') + 1:]\n",
    "        # Check if profile is new\n",
    "        if link not in profiles_scraped:\n",
    "            path = r\"twitter-data/\" + party + \"/\" + link + \".txt\"\n",
    "            # Scraper call:\n",
    "            # --since = date to which a user should be scraped.\n",
    "            #           use the current day - 1 to test before starting the actual run\n",
    "            # $link =   username. Thats the profile that will be scraped\n",
    "            # > $path = A path to where the output file is supposed to be saved.\n",
    "            #           In our case its './twitter-data/<name_of_party>/<username>.txt'\n",
    "            !snscrape --jsonl --since=\"2017-9-24\" twitter-user $link > $path\n",
    "            # Save the user to users that already have been scraped\n",
    "            profiles_scraped.append(link)\n",
    "            # Print out username for progress tracking\n",
    "            print(link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
