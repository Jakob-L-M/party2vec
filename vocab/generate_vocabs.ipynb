{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b990ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import glob\n",
    "import pandas as pd\n",
    "from os import path\n",
    "\n",
    "# stripped down spacy pretrained pipe, just tokenizer, lemmatizer, tagger and morphologizer in pipeline\n",
    "# since it is just used for tokenizing (tokenizer)\n",
    "# and lemmatizing (tokenizer, lemmatizer, tagger, morphologizer)\n",
    "nlp = spacy.load(\n",
    "    \"de_core_news_lg\", exclude=[\"tok2vec\", \"ner\", \"parser\", \"attribute_ruler\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23609e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create vocab from csv file, containing tweets\n",
    "# mode: should lemmas be used instead of token\n",
    "def create_vocab(file_name, mode=False):\n",
    "    vocab_count = defaultdict(int)\n",
    "    # reading file, only using the column containing the documents\n",
    "    df = pd.read_csv(file_name, sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "    # tokenizing/lemmatizing\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: nlp(str(x)))\n",
    "    # if token should be used:\n",
    "    if not mode:\n",
    "        # output file name\n",
    "        title = path.basename(file_name)[:-4] + \"_vocab_token.csv\"\n",
    "        # count occurences of each unique word\n",
    "        for line in df[\"text\"]:\n",
    "            for token in line:\n",
    "                vocab_count[token.text] += 1\n",
    "\n",
    "    # if lemma should be used:\n",
    "    if mode:\n",
    "        # output file name\n",
    "        title = path.basename(file_name)[:-4] + \"_vocab_lemma.csv\"\n",
    "        # count occurences of each unique lemma\n",
    "        for line in df[\"text\"]:\n",
    "            for token in line:\n",
    "                vocab_count[token.lemma_] += 1\n",
    "\n",
    "    # sorting by value and filter for min_word_count >= 3\n",
    "    vocab_count = {\n",
    "        k: v\n",
    "        for k, v in sorted(vocab_count.items(), key=lambda item: item[1], reverse=True)\n",
    "        if v > 2\n",
    "    }\n",
    "\n",
    "    # adding rank-column\n",
    "    df = pd.DataFrame.from_dict(data=vocab_count, orient=\"index\", columns=[\"count\"])\n",
    "    df[\"rank\"] = range(1, len(df) + 1)\n",
    "\n",
    "    # writing to file\n",
    "    df.to_csv(title, header=True, index_label=\"token\")\n",
    "    return \"{title} create with {count} words.\".format(\n",
    "        title=title, count=len(vocab_count)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a65db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for party in glob.glob(\"../cleaned-data/*.csv\"):\n",
    "    print(create_vocab(party, False))\n",
    "    print(create_vocab(party, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d40b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the vocabs of CDU and CSU to create the Union vocab\n",
    "# mode: should lemmas be used instead of token (see: create_vocab function)\n",
    "def create_vocab_union(mode=False):\n",
    "    vocab_count = defaultdict(int)\n",
    "\n",
    "    # if token should be used:\n",
    "    # importing dictionary from create_vocab (above)\n",
    "    if not mode:\n",
    "        title = \"union_vocab_token.csv\"\n",
    "        with open(\"CDU_vocab_token.csv\", mode=\"r\") as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            # skip first line\n",
    "            next(reader)\n",
    "            cdu = {rows[0]: rows[1] for rows in reader}\n",
    "        with open(\"CSU_vocab_token.csv\", mode=\"r\") as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            # skip first line\n",
    "            next(reader)\n",
    "            csu = {rows[0]: rows[1] for rows in reader}\n",
    "\n",
    "    # if lemma should be used:\n",
    "    # importing dictionary from create_vocab (above)\n",
    "    if mode:\n",
    "        title = \"union_vocab_lemma.csv\"\n",
    "        with open(\"CDU_vocab_lemma.csv\", mode=\"r\") as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            # skip first line\n",
    "            next(reader)\n",
    "            cdu = {rows[0]: rows[1] for rows in reader}\n",
    "        with open(\"CSU_vocab_lemma.csv\", mode=\"r\") as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            # skip first line\n",
    "            next(reader)\n",
    "            csu = {rows[0]: rows[1] for rows in reader}\n",
    "\n",
    "    # merging\n",
    "    for party in (cdu, csu):\n",
    "        for item in party:\n",
    "            vocab_count[item] += int(party[item])\n",
    "    # sorting by value and filter for min_word_count >= 3\n",
    "    vocab_count = {\n",
    "        k: v\n",
    "        for k, v in sorted(vocab_count.items(), key=lambda item: item[1], reverse=True)\n",
    "        if v > 2\n",
    "    }\n",
    "\n",
    "    # adding rank-column\n",
    "    df = pd.DataFrame.from_dict(data=vocab_count, orient=\"index\", columns=[\"count\"])\n",
    "    df[\"rank\"] = range(1, len(df) + 1)\n",
    "\n",
    "    # writing to file\n",
    "    df.to_csv(title, header=True, index_label=\"token\")\n",
    "    return \"{title} create with {count} words.\".format(\n",
    "        title=title, count=len(vocab_count)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f1ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging cdu and csu\n",
    "print(create_vocab_union(False))\n",
    "print(create_vocab_union(True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro2nlp",
   "language": "python",
   "name": "intro2nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
