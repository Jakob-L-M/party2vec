{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b990ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import glob\n",
    "import pandas as pd\n",
    "from os import path\n",
    "\n",
    "# stripped down spacy pretrained pipe, just tokenizer, lemmatizer, tagger and morphologizer in pipeline\n",
    "# since it is just used for tokenizing (tokenizer)\n",
    "# and lemmatizing (tokenizer, lemmatizer, tagger, morphologizer)\n",
    "nlp = spacy.load(\n",
    "    \"de_core_news_lg\", exclude=[\"tok2vec\", \"ner\", \"parser\", \"attribute_ruler\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23609e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create vocab from csv file, containing tweets\n",
    "# mode: should lemmas be used instead of token\n",
    "def create_vocab(file_name, mode=False):\n",
    "    vocab_count = defaultdict(int)\n",
    "    # reading file, only using the column containing the documents\n",
    "    df = pd.read_csv(file_name, sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "    # tokenizing/lemmatizing\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: nlp(str(x)))\n",
    "    # if token should be used:\n",
    "    if not mode:\n",
    "        # output file name\n",
    "        title = path.basename(file_name)[:-4] + \"_vocab_token.csv\"\n",
    "        # count occurences of each unique word\n",
    "        for line in df[\"text\"]:\n",
    "            for token in line:\n",
    "                vocab_count[token.text] += 1\n",
    "\n",
    "    # if lemma should be used:\n",
    "    if mode:\n",
    "        # output file name\n",
    "        title = path.basename(file_name)[:-4] + \"_vocab_lemma.csv\"\n",
    "        # count occurences of each unique lemma\n",
    "        for line in df[\"text\"]:\n",
    "            for token in line:\n",
    "                vocab_count[token.lemma_] += 1\n",
    "\n",
    "    # sorting by value and filter for min_word_count >= 3\n",
    "    vocab_count = {\n",
    "        k: v\n",
    "        for k, v in sorted(vocab_count.items(), key=lambda item: item[1], reverse=True)\n",
    "        if v > 2\n",
    "    }\n",
    "\n",
    "    # adding rank-column\n",
    "    df = pd.DataFrame.from_dict(data=vocab_count, orient=\"index\", columns=[\"count\"])\n",
    "    df[\"rank\"] = range(1, len(df) + 1)\n",
    "\n",
    "    # writing to file\n",
    "    df.to_csv(title, header=True, index_label=\"token\")\n",
    "    return \"{title} create with {count} words.\".format(\n",
    "        title=title, count=len(vocab_count)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a65db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for party in glob.glob(\"../cleaned-data/*.csv\"):\n",
    "    print(create_vocab(party, False))\n",
    "    print(create_vocab(party, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d40b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the vocabs of CDU and CSU to create the Union vocab\n",
    "# mode: should lemmas be used instead of token (see: create_vocab function)\n",
    "def create_vocab_union(mode=False):\n",
    "    vocab_count = defaultdict(int)\n",
    "\n",
    "    # if token should be used:\n",
    "    # importing dictionary from create_vocab (above)\n",
    "    if not mode:\n",
    "        title = \"union_vocab_token.csv\"\n",
    "        with open(\"CDU_vocab_token.csv\", mode=\"r\") as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            # skip first line\n",
    "            next(reader)\n",
    "            cdu = {rows[0]: rows[1] for rows in reader}\n",
    "        with open(\"CSU_vocab_token.csv\", mode=\"r\") as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            # skip first line\n",
    "            next(reader)\n",
    "            csu = {rows[0]: rows[1] for rows in reader}\n",
    "\n",
    "    # if lemma should be used:\n",
    "    # importing dictionary from create_vocab (above)\n",
    "    if mode:\n",
    "        title = \"union_vocab_lemma.csv\"\n",
    "        with open(\"CDU_vocab_lemma.csv\", mode=\"r\") as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            # skip first line\n",
    "            next(reader)\n",
    "            cdu = {rows[0]: rows[1] for rows in reader}\n",
    "        with open(\"CSU_vocab_lemma.csv\", mode=\"r\") as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            # skip first line\n",
    "            next(reader)\n",
    "            csu = {rows[0]: rows[1] for rows in reader}\n",
    "\n",
    "    # merging\n",
    "    for party in (cdu, csu):\n",
    "        for item in party:\n",
    "            vocab_count[item] += int(party[item])\n",
    "    # sorting by value and filter for min_word_count >= 3\n",
    "    vocab_count = {\n",
    "        k: v\n",
    "        for k, v in sorted(vocab_count.items(), key=lambda item: item[1], reverse=True)\n",
    "        if v > 2\n",
    "    }\n",
    "\n",
    "    # adding rank-column\n",
    "    df = pd.DataFrame.from_dict(data=vocab_count, orient=\"index\", columns=[\"count\"])\n",
    "    df[\"rank\"] = range(1, len(df) + 1)\n",
    "\n",
    "    # writing to file\n",
    "    df.to_csv(title, header=True, index_label=\"token\")\n",
    "    return \"{title} create with {count} words.\".format(\n",
    "        title=title, count=len(vocab_count)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f1ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging cdu and csu\n",
    "print(create_vocab_union(False))\n",
    "print(create_vocab_union(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0b4633",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(columns=['text'])\n",
    "\n",
    "afd = pd.read_csv('../cleaned-data/AfD.csv', sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "cdu = pd.read_csv('../cleaned-data/CDU.csv', sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "csu = pd.read_csv('../cleaned-data/CSU.csv', sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "fdp = pd.read_csv('../cleaned-data/FDP.csv', sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "gru = pd.read_csv('../cleaned-data/GRÜNE.csv', sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "lin = pd.read_csv('../cleaned-data/LINKE.csv', sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "spd = pd.read_csv('../cleaned-data/GRÜNE.csv', sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "\n",
    "data = data.append(afd, ignore_index=True)\n",
    "data = data.append(cdu, ignore_index=True)\n",
    "data = data.append(csu, ignore_index=True)\n",
    "data = data.append(fdp, ignore_index=True)\n",
    "data = data.append(gru, ignore_index=True)\n",
    "data = data.append(lin, ignore_index=True)\n",
    "data = data.append(spd, ignore_index=True)\n",
    "\n",
    "data[\"text\"] = data[\"text\"].apply(lambda x: nlp(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef93c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b2aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_count = defaultdict(int)\n",
    "title = \"all_lemma_vocab_token.csv\"\n",
    "\n",
    "for line in data[\"text\"]:\n",
    "    for token in line:\n",
    "        vocab_count[token.lemma_] += 1\n",
    "\n",
    "vocab_count = {\n",
    "    k: v\n",
    "    for k, v in sorted(vocab_count.items(), key=lambda item: item[1], reverse=True)\n",
    "    if v > 2\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(data=vocab_count, orient=\"index\", columns=[\"count\"])\n",
    "df[\"rank\"] = range(1, len(df) + 1)\n",
    "\n",
    "df.to_csv(title, header=True, index_label=\"token\")\n",
    "\"{title} create with {count} words.\".format(title=title, count=len(vocab_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8a24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_user(seq): \n",
    "    seen = 0\n",
    "    def inc():\n",
    "        nonlocal seen \n",
    "        seen = seen + 1\n",
    "        return seen == 1\n",
    "    return [x for x in seq if (x != \"user\" or inc())]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bdcdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"reduced_user\"] = data[\"text\"].apply(lambda x: remove_user([y.text for y in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ddaa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c3ede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_count = defaultdict(int)\n",
    "title = \"reduced_user_vocab_token.csv\"\n",
    "\n",
    "for line in data[\"reduced_user\"]:\n",
    "    for token in line:\n",
    "            vocab_count[token] += 1\n",
    "\n",
    "vocab_count = {\n",
    "    k: v\n",
    "    for k, v in sorted(vocab_count.items(), key=lambda item: item[1], reverse=True)\n",
    "    if v > 2\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(data=vocab_count, orient=\"index\", columns=[\"count\"])\n",
    "df[\"rank\"] = range(1, len(df) + 1)\n",
    "\n",
    "df.to_csv(title, header=True, index_label=\"token\")\n",
    "\"{title} create with {count} words.\".format(title=title, count=len(vocab_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eee3f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_party(seq):\n",
    "    return [x for x in seq if x.lower() not in [\"afd\",\"cdu\",\"csu\",\"linke\",\"fdp\",\"gruene\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4977bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"remove_party\"] = data[\"text\"].apply(lambda x: remove_party([y.text for y in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44515183",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_count = defaultdict(int)\n",
    "title = \"remove_party_vocab_token.csv\"\n",
    "\n",
    "for line in data[\"remove_party\"]:\n",
    "    for token in line:\n",
    "            vocab_count[token] += 1\n",
    "\n",
    "vocab_count = {\n",
    "    k: v\n",
    "    for k, v in sorted(vocab_count.items(), key=lambda item: item[1], reverse=True)\n",
    "    if v > 2\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(data=vocab_count, orient=\"index\", columns=[\"count\"])\n",
    "df[\"rank\"] = range(1, len(df) + 1)\n",
    "\n",
    "df.to_csv(title, header=True, index_label=\"token\")\n",
    "\"{title} create with {count} words.\".format(title=title, count=len(vocab_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro2nlp",
   "language": "python",
   "name": "intro2nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
