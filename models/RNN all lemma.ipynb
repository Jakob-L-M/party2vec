{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c03eb6",
   "metadata": {},
   "source": [
    "# Notebook for LSTM - Model Testing\n",
    "\n",
    "see \"RNN all data\" notebook\n",
    "\n",
    "contains similar test, only with the \"vector_all_data_lemma_300d-3-5.tsv\"-matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import import_ipynb\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load(\n",
    "    \"de_core_news_lg\", exclude=[\"tok2vec\", \"ner\", \"parser\", \"attribute_ruler\"]\n",
    ")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import make_sampling_table, pad_sequences\n",
    "from tensorflow.keras import Model, Sequential, Input\n",
    "from tensorflow.keras.layers import (\n",
    "    Dot,\n",
    "    Embedding,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    GlobalAveragePooling1D,\n",
    "    LSTM,\n",
    "    concatenate,\n",
    "    Dropout,\n",
    "    Bidirectional,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d24f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "data = pd.DataFrame(columns=['tweet', 'party'])\n",
    "\n",
    "afd = pd.read_csv('../cleaned-data/AfD.csv', quoting=csv.QUOTE_NONE)['text']\n",
    "afd = pd.DataFrame([[i, 0] for i in afd], columns=['tweet', 'party'])\n",
    "\n",
    "data = data.append(afd, ignore_index=True)\n",
    "\n",
    "cdu = pd.read_csv('../cleaned-data/CDU.csv',quoting=csv.QUOTE_NONE)['text']\n",
    "csu = pd.read_csv('../cleaned-data/CSU.csv',quoting=csv.QUOTE_NONE)['text']\n",
    "\n",
    "cdu = pd.DataFrame([[i, 1] for i in cdu], columns=['tweet', 'party'])\n",
    "csu = pd.DataFrame([[i, 1] for i in csu], columns=['tweet', 'party'])\n",
    "\n",
    "data = data.append(cdu, ignore_index=True)\n",
    "data = data.append(csu, ignore_index=True)\n",
    "\n",
    "fdp = pd.read_csv('../cleaned-data/FDP.csv',quoting=csv.QUOTE_NONE)['text']\n",
    "fdp = pd.DataFrame([[i, 2] for i in fdp], columns=['tweet', 'party'])\n",
    "\n",
    "data = data.append(fdp, ignore_index=True)\n",
    "\n",
    "gru = pd.read_csv('../cleaned-data/GRÜNE.csv',quoting=csv.QUOTE_NONE)['text']\n",
    "gru = pd.DataFrame([[i, 3] for i in gru], columns=['tweet', 'party'])\n",
    "\n",
    "data = data.append(gru, ignore_index=True)\n",
    "\n",
    "lin = pd.read_csv('../cleaned-data/LINKE.csv',quoting=csv.QUOTE_NONE)['text']\n",
    "lin = pd.DataFrame([[i, 4] for i in lin], columns=['tweet', 'party'])\n",
    "\n",
    "data = data.append(lin, ignore_index=True)\n",
    "\n",
    "spd = pd.read_csv('../cleaned-data/SPD.csv',quoting=csv.QUOTE_NONE)['text']\n",
    "spd = pd.DataFrame([[i, 5] for i in spd], columns=['tweet', 'party'])\n",
    "\n",
    "data = data.append(spd, ignore_index=True)\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7572a668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading vocab and matrix\n",
    "words = []\n",
    "em_matrix = np.genfromtxt(fname = \"../word_embedding/embeddings/vector_all_data_lemma_300d-3-5.tsv\", delimiter = \"\\t\" )       \n",
    "with open(\"../vocab/all_lemma_vocab_token.csv\", mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    next(reader) #skip header\n",
    "    vocab = {rows[0]:int(rows[2]) for rows in reader}\n",
    "# adding unknown-token for new words\n",
    "vocab['UNK'] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dcc0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming data\n",
    "# 1.) lemmatizing\n",
    "# 2.) padding to length 50\n",
    "data['vectors'] = data['tweet'].apply([lambda x: [vocab[y.lemma_] if y.lemma_ in vocab else vocab['UNK'] for y in nlp(str(x))]])\n",
    "data['vectors'] = data['vectors'].apply(lambda x: pad_sequences([x], maxlen=50, dtype=int, padding='post',value=0)[-1])\n",
    "data['len'] = data['vectors'].apply(lambda x: len(x))\n",
    "data_len = data['len'].max()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfde5154",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preparing label: transforming integer to vector\n",
    "y = tf.keras.utils.to_categorical(data['party'].to_numpy())\n",
    "y\n",
    "# preparing data (dataframe to np.array) \n",
    "X = np.array([np.array(x) for x in data['vectors']])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab9d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into train/test/validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dc9fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning model 1 (see RNN_all_data notebook)\n",
    "class RnnModel():\n",
    "\n",
    "    def __init__(self, embedding_matrix, embedding_dim, max_len):\n",
    "        \n",
    "        inp1 = Input(shape=(max_len,))\n",
    "        x = Embedding(embedding_matrix.shape[0], embedding_dim, weights=[embedding_matrix])(inp1)\n",
    "        x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "        x = Bidirectional(LSTM(64))(x)\n",
    "        x = Dense(128, activation=\"relu\")(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(64, activation=\"relu\")(x)\n",
    "        x = Dense(6, activation=\"softmax\")(x)    \n",
    "        model = Model(inputs=inp1, outputs=x)\n",
    "        model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])\n",
    "        self.model = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d6e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning model 2 (see RNN_all_data notebook)\n",
    "class RnnModel2():\n",
    "\n",
    "    def __init__(self, embedding_matrix, embedding_dim, max_len):\n",
    "        \n",
    "        inp1 = Input(shape=(max_len,))\n",
    "        x = Embedding(embedding_matrix.shape[0], embedding_dim, weights=[embedding_matrix])(inp1)\n",
    "        x = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
    "        x = Bidirectional(LSTM(128))(x)\n",
    "        x = Dense(256, activation=\"relu\")(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(64, activation=\"relu\")(x)\n",
    "        x = Dense(6, activation=\"softmax\")(x)    \n",
    "        model = Model(inputs=inp1, outputs=x)\n",
    "        model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4cae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 1 for lemma matrix\n",
    "m2 = RnnModel2(em_matrix, 300, 50)\n",
    "h2 = m2.model.fit(X_train, y_train, epochs = 5, batch_size = 512, verbose = 1, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e330bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing test 1\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16,7))\n",
    "# Block to evaluate training data\n",
    "yhat_test = m2.model.predict(X_test)\n",
    "yhat_test = np.argmax(yhat_test, axis=1)\n",
    "\n",
    "y_label_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "mat = confusion_matrix(y_label_test, yhat_test)\n",
    "df = pd.DataFrame(mat, index = [\"AfD\", \"Union\", \"FDP\", \"Grüne\", \"Linke\", \"SPD\"],\n",
    "                  columns = [\"AfD\", \"Union\", \"FDP\", \"Grüne\", \"Linke\", \"SPD\"])\n",
    "\n",
    "sn.heatmap(df, annot=True ,cmap='Blues', fmt='g', ax=axes[0]).set_title('Test Data');\n",
    "\n",
    "\n",
    "# Block to evaluate test data\n",
    "yhat_train = m2.model.predict(X_train)\n",
    "yhat_train = np.argmax(yhat_train, axis=1)\n",
    "\n",
    "y_label_train = np.argmax(y_train, axis=1)\n",
    "\n",
    "mat = confusion_matrix(y_label_train, yhat_train)\n",
    "df = pd.DataFrame(mat, index = [\"AfD\", \"Union\", \"FDP\", \"Grüne\", \"Linke\", \"SPD\"],\n",
    "                  columns = [\"AfD\", \"Union\", \"FDP\", \"Grüne\", \"Linke\", \"SPD\"])\n",
    "\n",
    "sn.heatmap(df, annot=True ,cmap='Blues', fmt='g', ax=axes[1]).set_title('Train Data');\n",
    "plt.show()\n",
    "print('Party: \\t Test \\t Train\\nAfd:\\t',sum(y_label_test == 0), \"\\t\" , sum(y_label_train == 0))\n",
    "print('Union:\\t',sum(y_label_test == 1), \"\\t\" , sum(y_label_train == 1))\n",
    "print('FDP:\\t',sum(y_label_test == 2), \"\\t\" , sum(y_label_train == 2))\n",
    "print('Grüne:\\t',sum(y_label_test == 3), \"\\t\" , sum(y_label_train == 3))\n",
    "print('Linke:\\t',sum(y_label_test == 4), \"\\t\" , sum(y_label_train == 4))\n",
    "print('SPD:\\t',sum(y_label_test == 5), \"\\t\" , sum(y_label_train == 5))\n",
    "print('\\nAcc:\\t', \"{:2.2f}%\".format(accuracy_score(y_label_test,yhat_test)*100), \"{:2.2f}%\".format(accuracy_score(y_label_train,yhat_train)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0c41a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning model 4 (see RNN_all_data notebook)\n",
    "class RnnModel4():\n",
    "\n",
    "    def __init__(self, embedding_matrix, embedding_dim, max_len):\n",
    "        \n",
    "        inp1 = Input(shape=(max_len,))\n",
    "        x = Embedding(embedding_matrix.shape[0], embedding_dim, weights=[embedding_matrix])(inp1)\n",
    "        x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "        x = Bidirectional(LSTM(64))(x)\n",
    "        x = Dense(128, activation=\"relu\")(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(128, activation=\"relu\")(x)\n",
    "        x = Dense(64, activation=\"relu\")(x)\n",
    "        x = Dense(6, activation=\"softmax\")(x)    \n",
    "        model = Model(inputs=inp1, outputs=x)\n",
    "        model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f21ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing test 2\n",
    "m5 = RnnModel4(em_matrix, 300, 50)\n",
    "h5 = m5.model.fit(X_train, y_train, epochs = 5, batch_size = 512, verbose = 1, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e747bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing test 2\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16,7))\n",
    "# Block to evaluate training data\n",
    "yhat_test = m5.model.predict(X_test)\n",
    "yhat_test = np.argmax(yhat_test, axis=1)\n",
    "\n",
    "y_label_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "mat = confusion_matrix(y_label_test, yhat_test)\n",
    "df = pd.DataFrame(mat, index = [\"AfD\", \"Union\", \"FDP\", \"Grüne\", \"Linke\", \"SPD\"],\n",
    "                  columns = [\"AfD\", \"Union\", \"FDP\", \"Grüne\", \"Linke\", \"SPD\"])\n",
    "\n",
    "sn.heatmap(df, annot=True ,cmap='Blues', fmt='g', ax=axes[0]).set_title('Test Data');\n",
    "\n",
    "\n",
    "# Block to evaluate test data\n",
    "yhat_train = m5.model.predict(X_train)\n",
    "yhat_train = np.argmax(yhat_train, axis=1)\n",
    "\n",
    "y_label_train = np.argmax(y_train, axis=1)\n",
    "\n",
    "mat = confusion_matrix(y_label_train, yhat_train)\n",
    "df = pd.DataFrame(mat, index = [\"AfD\", \"Union\", \"FDP\", \"Grüne\", \"Linke\", \"SPD\"],\n",
    "                  columns = [\"AfD\", \"Union\", \"FDP\", \"Grüne\", \"Linke\", \"SPD\"])\n",
    "\n",
    "sn.heatmap(df, annot=True ,cmap='Blues', fmt='g', ax=axes[1]).set_title('Train Data');\n",
    "plt.show()\n",
    "print('Party: \\t Test \\t Train\\nAfd:\\t',sum(y_label_test == 0), \"\\t\" , sum(y_label_train == 0))\n",
    "print('Union:\\t',sum(y_label_test == 1), \"\\t\" , sum(y_label_train == 1))\n",
    "print('FDP:\\t',sum(y_label_test == 2), \"\\t\" , sum(y_label_train == 2))\n",
    "print('Grüne:\\t',sum(y_label_test == 3), \"\\t\" , sum(y_label_train == 3))\n",
    "print('Linke:\\t',sum(y_label_test == 4), \"\\t\" , sum(y_label_train == 4))\n",
    "print('SPD:\\t',sum(y_label_test == 5), \"\\t\" , sum(y_label_train == 5))\n",
    "print('\\nAcc:\\t', \"{:2.2f}%\".format(accuracy_score(y_label_test,yhat_test)*100), \"{:2.2f}%\".format(accuracy_score(y_label_train,yhat_train)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21be6928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning model 3 (see RNN_all_data notebook)\n",
    "class RnnModel5():\n",
    "\n",
    "    def __init__(self, embedding_matrix, embedding_dim, max_len):\n",
    "        \n",
    "        inp1 = Input(shape=(max_len,))\n",
    "        x = Embedding(embedding_matrix.shape[0], embedding_dim, weights=[embedding_matrix])(inp1)\n",
    "        x = Bidirectional(LSTM(512, return_sequences=True))(x)\n",
    "        x = Bidirectional(LSTM(256))(x)\n",
    "        x = Dense(256, activation=\"relu\")(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(256, activation=\"relu\")(x)\n",
    "        x = Dense(64, activation=\"relu\")(x)\n",
    "        x = Dense(6, activation=\"softmax\")(x)    \n",
    "        model = Model(inputs=inp1, outputs=x)\n",
    "        model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dedb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 3 for lemma matrix\n",
    "m6 = RnnModel5(em_matrix, 300, 50)\n",
    "h6 = m6.model.fit(X_train, y_train, epochs = 5, batch_size = 512, verbose = 1, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3497bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing test 3\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16,7))\n",
    "# Block to evaluate training data\n",
    "yhat_test = m6.model.predict(X_test)\n",
    "yhat_test = np.argmax(yhat_test, axis=1)\n",
    "\n",
    "y_label_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "mat = confusion_matrix(y_label_test, yhat_test)\n",
    "df = pd.DataFrame(mat, index = [\"AfD\", \"Union\", \"FDP\", \"Grüne\", \"Linke\", \"SPD\"],\n",
    "                  columns = [\"AfD\", \"Union\", \"FDP\", \"Grüne\", \"Linke\", \"SPD\"])\n",
    "\n",
    "sn.heatmap(df, annot=True ,cmap='Blues', fmt='g', ax=axes[0]).set_title('Test Data');\n",
    "\n",
    "\n",
    "# Block to evaluate test data\n",
    "yhat_train = m6.model.predict(X_train)\n",
    "yhat_train = np.argmax(yhat_train, axis=1)\n",
    "\n",
    "y_label_train = np.argmax(y_train, axis=1)\n",
    "\n",
    "mat = confusion_matrix(y_label_train, yhat_train)\n",
    "df = pd.DataFrame(mat, index = [\"AfD\", \"Union\", \"FDP\", \"Grüne\", \"Linke\", \"SPD\"],\n",
    "                  columns = [\"AfD\", \"Union\", \"FDP\", \"Grüne\", \"Linke\", \"SPD\"])\n",
    "\n",
    "sn.heatmap(df, annot=True ,cmap='Blues', fmt='g', ax=axes[1]).set_title('Train Data');\n",
    "plt.show()\n",
    "print('Party: \\t Test \\t Train\\nAfd:\\t',sum(y_label_test == 0), \"\\t\" , sum(y_label_train == 0))\n",
    "print('Union:\\t',sum(y_label_test == 1), \"\\t\" , sum(y_label_train == 1))\n",
    "print('FDP:\\t',sum(y_label_test == 2), \"\\t\" , sum(y_label_train == 2))\n",
    "print('Grüne:\\t',sum(y_label_test == 3), \"\\t\" , sum(y_label_train == 3))\n",
    "print('Linke:\\t',sum(y_label_test == 4), \"\\t\" , sum(y_label_train == 4))\n",
    "print('SPD:\\t',sum(y_label_test == 5), \"\\t\" , sum(y_label_train == 5))\n",
    "print('\\nAcc:\\t', \"{:2.2f}%\".format(accuracy_score(y_label_test,yhat_test)*100), \"{:2.2f}%\".format(accuracy_score(y_label_train,yhat_train)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0827bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export test 1\n",
    "S = \"lemma_{mod}_ep{epo}_acc{acc}_valacc{valacc}\".format(\n",
    "        mod='Rnn2',\n",
    "        epo=5,\n",
    "        acc=\"{:2.2f}\".format(h2.history[\"accuracy\"][-1] * 100),\n",
    "        valacc=\"{:2.2f}\".format(h2.history[\"val_accuracy\"][-1]*100),\n",
    "    )\n",
    "m2.model.save_weights('models/'+S+'/model_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f521823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export test 2\n",
    "S = \"lemma_{mod}_ep{epo}_acc{acc}_valacc{valacc}\".format(\n",
    "        mod='Rnn4',\n",
    "        epo=5,\n",
    "        acc=\"{:2.2f}\".format(h5.history[\"accuracy\"][-1] * 100),\n",
    "        valacc=\"{:2.2f}\".format(h5.history[\"val_accuracy\"][-1]*100),\n",
    "    )\n",
    "m5.model.save_weights('models/'+S+'/model_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb91f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export test 3\n",
    "S = \"lemma_{mod}_ep{epo}_acc{acc}_valacc{valacc}\".format(\n",
    "        mod='Rnn5',\n",
    "        epo=5,\n",
    "        acc=\"{:2.2f}\".format(h6.history[\"accuracy\"][-1] * 100),\n",
    "        valacc=\"{:2.2f}\".format(h6.history[\"val_accuracy\"][-1]*100),\n",
    "    )\n",
    "m6.model.save_weights('models/'+S+'/model_weights')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro2nlp",
   "language": "python",
   "name": "intro2nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
