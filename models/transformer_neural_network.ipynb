{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we want to try to use a Neural Network to predict the correct party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "# exclude everything we dont need for faster performance\n",
    "nlp = spacy.load(\"de_core_news_lg\", exclude=['tagger', 'morphologizer', 'parser', 'senter', 'ner', 'attribute_ruler', 'lemmatizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading cleand data\n",
    "The following Block creates a data Frame in which each tweets is labeled with its corresponding party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(columns=['tweet', 'party'])\n",
    "\n",
    "afd = pd.read_csv('../cleaned-data/AfD.csv')['text']\n",
    "afd = pd.DataFrame([[i, 0] for i in afd], columns=['tweet', 'party'])\n",
    "\n",
    "data = data.append(afd, ignore_index=True)\n",
    "\n",
    "cdu = pd.read_csv('../cleaned-data/CDU.csv')['text']\n",
    "csu = pd.read_csv('../cleaned-data/CSU.csv')['text']\n",
    "\n",
    "cdu = pd.DataFrame([[i, 1] for i in cdu], columns=['tweet', 'party'])\n",
    "csu = pd.DataFrame([[i, 1] for i in csu], columns=['tweet', 'party'])\n",
    "\n",
    "data = data.append(cdu, ignore_index=True)\n",
    "data = data.append(csu, ignore_index=True)\n",
    "\n",
    "fdp = pd.read_csv('../cleaned-data/FDP.csv')['text']\n",
    "fdp = pd.DataFrame([[i, 2] for i in fdp], columns=['tweet', 'party'])\n",
    "\n",
    "data = data.append(fdp, ignore_index=True)\n",
    "\n",
    "gru = pd.read_csv('../cleaned-data/GRÃœNE.csv')['text']\n",
    "gru = pd.DataFrame([[i, 3] for i in gru], columns=['tweet', 'party'])\n",
    "\n",
    "data = data.append(gru, ignore_index=True)\n",
    "\n",
    "lin = pd.read_csv('../cleaned-data/LINKE.csv')['text']\n",
    "lin = pd.DataFrame([[i, 4] for i in lin], columns=['tweet', 'party'])\n",
    "\n",
    "data = data.append(lin, ignore_index=True)\n",
    "\n",
    "spd = pd.read_csv('../cleaned-data/SPD.csv')['text']\n",
    "spd = pd.DataFrame([[i, 5] for i in spd], columns=['tweet', 'party'])\n",
    "\n",
    "data = data.append(spd, ignore_index=True)\n",
    "\n",
    "# Removing NaN. Those were probably tweets with only a link or emojis with dont include anything after cleaning\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming tweets to vectors\n",
    "For this step we are using the pre-trained de_core_news_lg spacy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [nlp(tweet).vector for tweet in tqdm(data['tweet'].to_numpy())]\n",
    "# We will save the vectors to a file sice its faster to read them in if we want to use them somewhere else.\n",
    "# The file is ~2.2GB\n",
    "np.savetxt('vector_tweets.out', X, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vector representation (300d)\n",
    "X = np.loadtxt('vector_tweets.out', delimiter=',')\n",
    "# load lable matrix. The function to_categorical will transform our labels, which are numbers from 0 to 5\n",
    "# to one-hot encoded vectors. So 0 -> [1, 0, 0, 0, 0, 0], 1 -> [0, 1, 0, 0, 0, 0], 2 -> ....\n",
    "y = to_categorical(data['party'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will only use tweets with at least 10 words.\n",
    "# Tweets with less words can hardly have any\n",
    "filter_tweets = np.array([len(tweet.split(\" \")) >= 7 for tweet in data['tweet']], dtype=np.bool_)\n",
    "\n",
    "X_wo_short = X[filter_tweets]\n",
    "y_wo_short = y[filter_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_wo_short, y_wo_short, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Dense(300, activation='relu'))\n",
    "model1.add(Dropout(0.25))\n",
    "model1.add(Dense(6, activation='softmax'))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(150, activation='relu'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(150, activation='relu'))\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(Dense(75, activation='relu'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(6, activation='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Dense(75, activation='relu'))\n",
    "model3.add(Dropout(0.1))\n",
    "model3.add(Dense(50, activation='relu'))\n",
    "model3.add(Dropout(0.075))\n",
    "model3.add(Dense(50, activation='relu'))\n",
    "model3.add(Dropout(0.075))\n",
    "model3.add(Dense(50, activation='relu'))\n",
    "model3.add(Dropout(0.05))\n",
    "model3.add(Dense(6, activation='softmax'))\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ep = 100\n",
    "hist1 = model1.fit(X_train, y_train, epochs=ep, batch_size=512, verbose=2,\n",
    "                validation_data=(X_test, y_test));\n",
    "hist2 = model2.fit(X_train, y_train, epochs=ep, batch_size=512, verbose=2,\n",
    "                validation_data=(X_test, y_test));\n",
    "hist3 = model3.fit(X_train, y_train, epochs=ep, batch_size=512, verbose=2,\n",
    "                validation_data=(X_test, y_test));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "print(\"Model 1: 300-300-6\")\n",
    "vis_hist(hist1)\n",
    "vis_confusion_mat(model1, X_test, X_train, y_test, y_train)\n",
    "vis_classification_distribution(model1, X_test, y_test)\n",
    "# vis_classification_distribution(model1, X_train, y_train)\n",
    "\n",
    "print(\"Model 2: 300-150-150-75-6\")\n",
    "vis_hist(hist2)\n",
    "vis_confusion_mat(model2, X_test, X_train, y_test, y_train)\n",
    "vis_classification_distribution(model2, X_test, y_test)\n",
    "# vis_classification_distribution(model2, X_train, y_train)\n",
    "\n",
    "print(\"Model 3: 300-75-50-50-50-6\")\n",
    "vis_hist(hist3)\n",
    "vis_confusion_mat(model3, X_test, X_train, y_test, y_train)\n",
    "vis_classification_distribution(model3, X_test, y_test)\n",
    "# vis_classification_distribution(model3, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to make a prediction based on a combination of models aka a forest\n",
    "def predict(models, X):\n",
    "    predictions = []\n",
    "    for m in models:\n",
    "        predictions.append(m.predict(X))\n",
    "    p = np.array(predictions)\n",
    "    p = np.sum(p, axis=0)\n",
    "    p = np.exp(p)\n",
    "    p = p.T/np.sum(p, axis=1)\n",
    "    return p.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Acc, Remetricsl, Prec and F1 for each party\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "yhat_test = np.argmax(predict([model1, model2, model3], X_test), axis=1)\n",
    "y_label_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(classification_report(y_label_test, yhat_test, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(model, name):\n",
    "    result = []\n",
    "    for layer in model.layers:\n",
    "        if type(layer) == Dense:\n",
    "            weights = layer.get_weights()[0]\n",
    "            bias = layer.get_weights()[1]\n",
    "            result.append({'weights': weights.tolist(), 'bias': bias.tolist()})\n",
    "    with open(name+'.txt', 'w') as outfile:\n",
    "        json.dump(result, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "export_model(model3, 'model3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_hist(hist):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16,7))\n",
    "    a = axes.ravel()\n",
    "    a[0].plot(hist.history['loss'])\n",
    "    a[0].plot(hist.history['val_loss'])\n",
    "    a[0].set_title('Model loss')\n",
    "    a[0].set_ylabel('Loss')\n",
    "    a[0].set_xlabel('Epoch')\n",
    "    a[0].legend(['Train', 'Validation'], loc='upper left')\n",
    "    a[1].plot(hist.history['accuracy'])\n",
    "    a[1].plot(hist.history['val_accuracy'])\n",
    "    a[1].set_title('Model accuracy')\n",
    "    a[1].set_ylabel('Accuracy')\n",
    "    a[1].set_xlabel('Epoch')\n",
    "    a[1].legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def vis_confusion_mat(model, X_test, X_train, y_test, y_train):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16,7))\n",
    "    # Block to evaluate test data\n",
    "    yhat_test_prop = model.predict(X_test)\n",
    "    yhat_test = np.argmax(yhat_test_prop, axis=1)\n",
    "\n",
    "    y_label_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    mat = confusion_matrix(y_label_test, yhat_test)\n",
    "    df = pd.DataFrame(mat, index = [\"AfD\", \"Union\", \"FDP\", \"GrÃ¼ne\", \"Linke\", \"SPD\"],\n",
    "                      columns = [\"AfD\", \"Union\", \"FDP\", \"GrÃ¼ne\", \"Linke\", \"SPD\"])\n",
    "    # ax1.figure(figsize = (7,5))\n",
    "    sn.heatmap(df, annot=True ,cmap='Blues', fmt='g', ax=axes[0]).set_title('Test Data');\n",
    "    # ax1.show()\n",
    "\n",
    "    # Block to evaluate train data\n",
    "    yhat_train_prop = model.predict(X_train)\n",
    "    yhat_train = np.argmax(yhat_train_prop, axis=1)\n",
    "\n",
    "    y_label_train = np.argmax(y_train, axis=1)\n",
    "\n",
    "    mat = confusion_matrix(y_label_train, yhat_train)\n",
    "    df = pd.DataFrame(mat, index = [\"AfD\", \"Union\", \"FDP\", \"GrÃ¼ne\", \"Linke\", \"SPD\"],\n",
    "                      columns = [\"AfD\", \"Union\", \"FDP\", \"GrÃ¼ne\", \"Linke\", \"SPD\"])\n",
    "    # ax2.figure(figsize = (7,5))\n",
    "    sn.heatmap(df, annot=True ,cmap='Blues', fmt='g', ax=axes[1]).set_title('Train Data');\n",
    "    plt.show()\n",
    "    print('Party: \\t Test \\t Train\\nAfd:\\t',sum(y_label_test == 0), \"\\t\" , sum(y_label_train == 0))\n",
    "    print('Union:\\t',sum(y_label_test == 1), \"\\t\" , sum(y_label_train == 1))\n",
    "    print('FDP:\\t',sum(y_label_test == 2), \"\\t\" , sum(y_label_train == 2))\n",
    "    print('GrÃ¼ne:\\t',sum(y_label_test == 3), \"\\t\" , sum(y_label_train == 3))\n",
    "    print('Linke:\\t',sum(y_label_test == 4), \"\\t\" , sum(y_label_train == 4))\n",
    "    print('SPD:\\t',sum(y_label_test == 5), \"\\t\" , sum(y_label_train == 5))\n",
    "    print('\\nAcc:\\t', \"{:2.2f}%\".format(accuracy_score(y_label_test,yhat_test)*100), \"{:2.2f}%\".format(accuracy_score(y_label_train,yhat_train)*100))\n",
    "    \n",
    "def vis_classification_distribution(model, X_test, y_test):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16,7))\n",
    "    a = axes.ravel()\n",
    "    \n",
    "    yhat_test_prop = model.predict(X_test)\n",
    "    y_label_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    party_probs = [[], [], [], [], [], []]\n",
    "    for ind,val in enumerate(yhat_test_prop):\n",
    "        party = y_label_test[ind]\n",
    "        party_probs[party].append(val[party])\n",
    "\n",
    "    labels = [\"AfD\", \"Union\", \"FDP\", \"GrÃ¼ne\", \"Linke\", \"SPD\"]\n",
    "    for i,j in enumerate(party_probs):\n",
    "        a[i].hist(j, bins=20, density=True)\n",
    "        a[i].set_title(labels[i])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
