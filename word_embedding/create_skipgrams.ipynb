{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1a5082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.sequence import make_sampling_table, skipgrams\n",
    "from tensorflow.random import log_uniform_candidate_sampler as sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b5156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates skipgrams with 1 positive and varing amounts of negative samples using tensorflow,\n",
    "# wrapped as a pd.Dataframe\n",
    "def create_skipgrams_with_negative_samples(\n",
    "    list_sentence, n_size, neg_sam_size, vocab_size, seed, generate_table=True\n",
    "):\n",
    "    target_l, sample_l, label_l = [], [], []\n",
    "    # Sampling Table: Works only with sorted vocab, with 0 as non-word, 1 the most likely word in vocab,..., if no sorting =>\n",
    "    sampling_table = make_sampling_table(size=vocab_size) if generate_table else None\n",
    "    # iterating over each sentence in list_sentence\n",
    "    for sentence in tqdm(list_sentence):\n",
    "        # create positive n-grams for each word with range n_size\n",
    "        # if sampling_table = None, each word in range is selected\n",
    "        # due to negative_samples=0, all pairs are positive skipgrams in range n_size\n",
    "        # labels are discarded, since they are all positive (1)\n",
    "        pos_skipgrams, _ = skipgrams(\n",
    "            sequence=sentence,\n",
    "            vocabulary_size=vocab_size,\n",
    "            window_size=n_size,\n",
    "            shuffle=True,\n",
    "            sampling_table=sampling_table,\n",
    "            seed=seed,\n",
    "            negative_samples=0,\n",
    "        )\n",
    "        # for each positiv sample, neg_sam_size negative samples are randomly generated\n",
    "        for target, context_word in pos_skipgrams:\n",
    "            # for collection of data: saving of target\n",
    "            target_l.append(target)\n",
    "            # for collection of data: saving of labels: 1 positive label and neg_sam_size negative\n",
    "            label = tf.constant([1] + [0]*neg_sam_size, dtype=\"int64\")\n",
    "            label_l.append(label)\n",
    "            \n",
    "            context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"),  1)\n",
    "            # generating unique, negative samples\n",
    "            negative_sampling_candidates, _, _ = sampler(\n",
    "                true_classes=context_class,\n",
    "                num_true=1,\n",
    "                num_sampled=neg_sam_size,\n",
    "                unique=True,\n",
    "                range_max=vocab_size,\n",
    "                seed=seed,\n",
    "                name=\"negative_sampling\",\n",
    "            )\n",
    "            negative_sampling_candidates = tf.expand_dims(\n",
    "              negative_sampling_candidates, 1)\n",
    "            context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "            # for collection of data: saving of context-word: 1 positive context and neg_sam_size context\n",
    "            sample_l.append(context)\n",
    "    return target_l, sample_l, label_l"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro2nlp",
   "language": "python",
   "name": "intro2nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
