{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0c631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import csv\n",
    "import pandas as pd\n",
    "import import_ipynb\n",
    "import create_skipgrams\n",
    "import glob\n",
    "import gc\n",
    "import spacy\n",
    "import io\n",
    "nlp = spacy.load(\n",
    "    \"de_core_news_lg\", exclude=[\"tok2vec\", \"ner\", \"parser\", \"attribute_ruler\"]\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import make_sampling_table\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dot, Embedding, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1fdd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "# 4 layers\n",
    "\n",
    "class Word2Vec(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, negative_samples):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        \n",
    "        # 1.) Embedding layer: gets embedding for the target word\n",
    "        self.target_embedding = Embedding(\n",
    "            vocab_size, embedding_dim, input_length=1, name=\"w2v_embedding\"\n",
    "        )\n",
    "        # 2.) Embedding layer: gets embedding for the context word\n",
    "        self.context_embedding = Embedding(\n",
    "            vocab_size, embedding_dim, input_length=negative_samples + 1\n",
    "        )\n",
    "        # 3.) Dot-Product for target and context embedding\n",
    "        self.dots = Dot(axes=(3, 2))\n",
    "        # 4.) flatten (embedding dim * embeddings dim -> embeddings dim)\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        word_emb = self.target_embedding(target)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        dots = self.dots([context_emb, word_emb])\n",
    "        return self.flatten(dots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af9a73d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802a81dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create word embedding, with data (as number vector), vocab with word assotiated with its number,\n",
    "# skipgram-size, negative samplge and dim as parameters for the embedding (as list for the creation of multiple embeddings from the same data),\n",
    "# label as the name for saving and a seed\n",
    "def create_word_embeddings(\n",
    "    data, vocab, skipgram_size, negative_samples, dim, labeling, seed=1234, epochs=25\n",
    "):\n",
    "    # checking inputs are lists\n",
    "    assert (\n",
    "        isinstance(skipgram_size, list)\n",
    "        and isinstance(negative_samples, list)\n",
    "        and isinstance(dim, list)\n",
    "    ), \"skipgram_size, negative_samples, dim are supposed to be lists\"\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    max_len_sen = max(len(elem) for elem in data)\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # padding sentences\n",
    "    sentences_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        data, maxlen=max_len_sen, dtype=int, padding=\"post\", value=0\n",
    "    )\n",
    "    history = []\n",
    "\n",
    "    # creating skipgrams\n",
    "    for sg_size in skipgram_size:\n",
    "        for ns in negative_samples:\n",
    "            target, sample, label = [],[],[]\n",
    "            for b in batch(data, 10000):\n",
    "                t,s,l  = create_skipgrams.create_skipgrams_with_negative_samples(\n",
    "                b, sg_size, ns, vocab_size, seed, generate_table=True\n",
    "                )\n",
    "                target = target + t\n",
    "                sample = sample + s\n",
    "                label = label + l\n",
    "                gc.collect()\n",
    "            print(\n",
    "                \"generated data for skipgram: \"\n",
    "                + str(sg_size)\n",
    "                + \" and negative samples: \"\n",
    "                + str(ns)\n",
    "            )\n",
    "\n",
    "            BATCH_SIZE = 1024\n",
    "            BUFFER_SIZE = 10000\n",
    "\n",
    "            # creating dataset, shuffling and prefetching\n",
    "            dataset = tf.data.Dataset.from_tensor_slices(((target, sample), label))\n",
    "            dataset = dataset.shuffle(BUFFER_SIZE).batch(\n",
    "                BATCH_SIZE, drop_remainder=True\n",
    "            )\n",
    "            dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "            for em_size in dim:\n",
    "\n",
    "                # embedding-model\n",
    "                word2vec = Word2Vec(vocab_size, em_size, ns + 1)\n",
    "                log_dir = (\n",
    "                    \"logs/fit/\"\n",
    "                    + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "                    + \"dim{dim}dnegsamp{ns}skipgram{sks}\".format(\n",
    "                        dim=em_size, ns=ns, sks=sg_size\n",
    "                    )\n",
    "                )\n",
    "                tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "                    log_dir=log_dir, histogram_freq=1\n",
    "                )\n",
    "                print(\n",
    "                    \"started compilation with dim{dim}, negative_samples: {ns}, skipgram_size: {sks}\".format(\n",
    "                        dim=em_size, ns=ns, sks=sg_size\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # create embedding\n",
    "                word2vec.compile(\n",
    "                    optimizer=\"adam\",\n",
    "                    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                    metrics=[\"accuracy\"],\n",
    "                )\n",
    "                h = word2vec.fit(\n",
    "                    dataset, epochs=epochs, callbacks=[tensorboard_callback]\n",
    "                )\n",
    "                history.append(h)\n",
    "\n",
    "                # get resulting vectors\n",
    "                weights = word2vec.get_layer(\"w2v_embedding\").get_weights()[0]\n",
    "\n",
    "                # saving embedding\n",
    "                vec_string = (\n",
    "                    labeling + str(em_size) + \"d-\" + str(sg_size) + \"-\" + str(ns)\n",
    "                )\n",
    "                out_v = io.open(\n",
    "                    \"embeddings\\\\vector_\" + vec_string + \".tsv\", \"w\", encoding=\"utf-8\"\n",
    "                )\n",
    "                out_m = io.open(\n",
    "                    \"embeddings\\\\metadata_\" + vec_string + \".tsv\", \"w\", encoding=\"utf-8\"\n",
    "                )\n",
    "                for index, word in enumerate(tqdm(vocab)):\n",
    "                    vec = weights[index]\n",
    "                    out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\")\n",
    "                    out_m.write(word + \"\\n\")\n",
    "                out_v.close()\n",
    "                out_m.close()\n",
    "    # returning history\n",
    "    return history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro2nlp",
   "language": "python",
   "name": "intro2nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
