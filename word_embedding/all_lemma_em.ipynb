{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98e4f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import import_ipynb\n",
    "import create_skipgrams\n",
    "import create_word_embeddings\n",
    "import spacy\n",
    "nlp = spacy.load(\n",
    "    \"de_core_news_lg\", exclude=[\"tok2vec\", \"ner\", \"parser\", \"attribute_ruler\"]\n",
    ")\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2675b4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "with open(\"../vocab/all_lemma_vocab_token.csv\", mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    next(reader) #skip header\n",
    "    vocab = {rows[0]:int(rows[2]) for rows in reader}\n",
    "vocab['UNK'] = len(vocab)\n",
    "vocab[''] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bed6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(columns=['text'])\n",
    "\n",
    "afd = pd.read_csv('../cleaned-data/AfD.csv', sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "cdu = pd.read_csv('../cleaned-data/CDU.csv', sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "csu = pd.read_csv('../cleaned-data/CSU.csv', sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "fdp = pd.read_csv('../cleaned-data/FDP.csv', sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "gru = pd.read_csv('../cleaned-data/GRÃœNE.csv', sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "lin = pd.read_csv('../cleaned-data/LINKE.csv', sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "spd = pd.read_csv('../cleaned-data/SPD.csv', sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "\n",
    "data = data.append(afd, ignore_index=True)\n",
    "data = data.append(cdu, ignore_index=True)\n",
    "data = data.append(csu, ignore_index=True)\n",
    "data = data.append(fdp, ignore_index=True)\n",
    "data = data.append(gru, ignore_index=True)\n",
    "data = data.append(lin, ignore_index=True)\n",
    "data = data.append(spd, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5781cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "del afd\n",
    "del cdu\n",
    "del csu\n",
    "del fdp\n",
    "del gru\n",
    "del lin\n",
    "del spd\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6acac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_seed = 1234\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6172c730",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data['tokens'] = data['text'].apply([lambda x: [vocab[y.lemma_] if y.lemma_ in vocab else vocab['UNK'] for y in nlp(str(x))]])\n",
    "data_list = data['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c86390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59106a3c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "create_word_embeddings.create_word_embeddings(\n",
    "    data_list, vocab, [3], [5], [300], \"all_data_lemma_\", used_seed, 15\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro2nlp",
   "language": "python",
   "name": "intro2nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
