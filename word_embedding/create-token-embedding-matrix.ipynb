{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d84a32",
   "metadata": {},
   "source": [
    "# Notebook the Generate different word embedding \n",
    "Parameter, that can be tested: skipgram size, # of negative samples, dimension of embedding vector\n",
    "\n",
    "this can be used to test create_word_embeddings.ipynb, after cleaning and vocab-creation\n",
    "\n",
    "see also create-lemma-embedding-matrix or create-noparty-embedding-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af27004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import import_ipynb\n",
    "import create_skipgrams\n",
    "import create_word_embeddings\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "nlp = spacy.load(\n",
    "    \"de_core_news_lg\", exclude=[\"tok2vec\", \"ner\", \"parser\", \"attribute_ruler\"]\n",
    ")\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3abeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab loading\n",
    "with open(\"../vocab/all_vocab_token.csv\", mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    next(reader) #skip header\n",
    "    vocab = {rows[0]:int(rows[2]) for rows in reader}\n",
    "vocab['UNK'] = len(vocab) # for new/unknown token\n",
    "vocab[''] = 0 # padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b645baf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collection\n",
    "data = pd.DataFrame(columns=['text'])\n",
    "\n",
    "afd = pd.read_csv('../cleaned-data/AfD.csv', sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "cdu = pd.read_csv('../cleaned-data/CDU.csv', sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "csu = pd.read_csv('../cleaned-data/CSU.csv', sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "fdp = pd.read_csv('../cleaned-data/FDP.csv', sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "gru = pd.read_csv('../cleaned-data/GRÃœNE.csv', sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "lin = pd.read_csv('../cleaned-data/LINKE.csv', sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "spd = pd.read_csv('../cleaned-data/SPD.csv', sep=\",\", quoting=csv.QUOTE_NONE, usecols=[2])\n",
    "\n",
    "data = data.append(afd, ignore_index=True)\n",
    "data = data.append(cdu, ignore_index=True)\n",
    "data = data.append(csu, ignore_index=True)\n",
    "data = data.append(fdp, ignore_index=True)\n",
    "data = data.append(gru, ignore_index=True)\n",
    "data = data.append(lin, ignore_index=True)\n",
    "data = data.append(spd, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3cd1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to free up ram\n",
    "del afd\n",
    "del cdu\n",
    "del csu\n",
    "del fdp\n",
    "del gru\n",
    "del lin\n",
    "del spd\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7083cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_seed = 1234\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef1dd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing using spacy\n",
    "data['tokens'] = data['text'].apply([lambda x: [vocab[y.text] if y.text in vocab else vocab['UNK'] for y in nlp(str(x))]])\n",
    "data_list = data['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f984ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to free up ram\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117d5cb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using create_word_embeddings.ipynb to create a word embedding with 3-wide skipgram window, 5 negative samples and \n",
    "# a embedding dimension of 300\n",
    "create_word_embeddings.create_word_embeddings(\n",
    "    data_list, vocab, [3], [5], [300], \"all_data_\", used_seed, 15\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro2nlp",
   "language": "python",
   "name": "intro2nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
